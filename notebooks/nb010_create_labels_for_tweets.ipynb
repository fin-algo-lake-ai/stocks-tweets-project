{"cells":[{"cell_type":"code","source":["# This notebook processes specified raw datasets with tweets for a specific set ot tickers\n","# The draft of preparation code taken from this article: https://arxiv.org/abs/2103.16388"],"metadata":{"id":"dGUy_yp588vM","executionInfo":{"status":"ok","timestamp":1671218434627,"user_tz":-180,"elapsed":204,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HkqDSyOVDpRD"},"source":["# Install, Import statements"]},{"cell_type":"code","source":["# NEW 2022-11\n","!python -V"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"td3_F8Onpb4M","executionInfo":{"status":"ok","timestamp":1671218435132,"user_tz":-180,"elapsed":269,"user":{"displayName":"AnT","userId":"07342426211356883844"}},"outputId":"b6d4f9ec-7272-4cef-9291-2c9c1ef187ed"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.8.16\n"]}]},{"cell_type":"code","source":["# Note: GPU is not required for this notebook\n","!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RCx5uZ_VpjvB","executionInfo":{"status":"ok","timestamp":1671218435498,"user_tz":-180,"elapsed":369,"user":{"displayName":"AnT","userId":"07342426211356883844"}},"outputId":"ce9b8765-57ff-4cc2-f163-2907c669e0e6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"]}]},{"cell_type":"code","source":["!pip install contractions\n","!pip install emoji\n","!pip install ekphrasis\n","!pip install yfinance --upgrade --no-cache-dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nGWqy0UlC5me","executionInfo":{"status":"ok","timestamp":1671218455587,"user_tz":-180,"elapsed":20092,"user":{"displayName":"AnT","userId":"07342426211356883844"}},"outputId":"0ddae2a0-9508-427a-9113-b296ccf1661d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting contractions\n","  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n","Collecting textsearch>=0.0.21\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Collecting pyahocorasick\n","  Downloading pyahocorasick-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (110 kB)\n","\u001b[K     |████████████████████████████████| 110 kB 5.3 MB/s \n","\u001b[?25hCollecting anyascii\n","  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n","\u001b[K     |████████████████████████████████| 287 kB 39.6 MB/s \n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-1.4.4 textsearch-0.0.24\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting emoji\n","  Downloading emoji-2.2.0.tar.gz (240 kB)\n","\u001b[K     |████████████████████████████████| 240 kB 4.9 MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=d1548ee467b130bc215f5f9decc6f5e3030c80d98b8f7336552192c69798e570\n","  Stored in directory: /root/.cache/pip/wheels/86/62/9e/a6b27a681abcde69970dbc0326ff51955f3beac72f15696984\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-2.2.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ekphrasis\n","  Downloading ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n","\u001b[K     |████████████████████████████████| 83 kB 1.7 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from ekphrasis) (4.64.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from ekphrasis) (1.21.6)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from ekphrasis) (3.7)\n","Collecting ujson\n","  Downloading ujson-5.6.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 1.2 MB/s \n","\u001b[?25hCollecting ftfy\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from ekphrasis) (3.2.2)\n","Collecting colorama\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from ekphrasis) (2.1.1)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->ekphrasis) (0.2.5)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ekphrasis) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ekphrasis) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->ekphrasis) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->ekphrasis) (7.1.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->ekphrasis) (2022.6.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->ekphrasis) (1.2.0)\n","Installing collected packages: ujson, ftfy, colorama, ekphrasis\n","Successfully installed colorama-0.4.6 ekphrasis-0.5.4 ftfy-6.1.1 ujson-5.6.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting yfinance\n","  Downloading yfinance-0.1.90-py2.py3-none-any.whl (29 kB)\n","Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.8/dist-packages (from yfinance) (4.9.2)\n","Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.8/dist-packages (from yfinance) (0.0.11)\n","Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.8/dist-packages (from yfinance) (1.4.4)\n","Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from yfinance) (1.3.5)\n","Collecting requests>=2.26\n","  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 6.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from yfinance) (1.21.6)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.0->yfinance) (2022.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.3.0->yfinance) (1.15.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (2.10)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (2.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n","Installing collected packages: requests, yfinance\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","Successfully installed requests-2.28.1 yfinance-0.1.90\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"ExecuteTime":{"end_time":"2022-11-28T13:34:40.724971Z","start_time":"2022-11-28T13:34:15.810635Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"_gXtdFlzndWh","outputId":"306f5a5e-3e85-454b-8d87-a11c8aeaa9af","executionInfo":{"status":"ok","timestamp":1671218464434,"user_tz":-180,"elapsed":8853,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["from collections import defaultdict\n","import contractions\n","from ekphrasis.classes.segmenter import Segmenter\n","import emoji\n","from datetime import datetime, timedelta, date, timezone\n","import itertools\n","import json\n","import numpy as np\n","import os\n","import pandas as pd\n","#from pandas_datareader import data as pdr\n","from pprint import pprint\n","import random\n","import re\n","import requests \n","import string\n","import sys\n","#import tensorflow as tf  # NEW 2022-11: commented\n","#import tensorflow_hub as hub  # NEW 2022-11: commented\n","#from tensorflow import keras # NEW 2022-11: commented\n","import torch\n","from tqdm.notebook import tqdm  # NEW 2022-11\n","import yfinance\n","\n","# NLTK section\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","# from nltk.tokenize import word_tokenize\n","from nltk.tokenize import TweetTokenizer \n","from nltk.corpus import stopwords as sw\n","from nltk.tokenize import word_tokenize \n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","from nltk.stem import PorterStemmer"]},{"cell_type":"markdown","source":["# Mounts"],"metadata":{"id":"R6DviY4hFUu0"}},{"cell_type":"code","source":["# Reqiured for mounting\n","import google.colab\n","google.colab.drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kPYyKH7kFO9B","executionInfo":{"status":"ok","timestamp":1671218483653,"user_tz":-180,"elapsed":19245,"user":{"displayName":"AnT","userId":"07342426211356883844"}},"outputId":"56568bd7-5481-492c-fcf7-c524838bc5d1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["assert os.path.isdir('/content/drive/MyDrive')"],"metadata":{"id":"yU9rhd-gG6Dr","executionInfo":{"status":"ok","timestamp":1671218483654,"user_tz":-180,"elapsed":7,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Required for getting files by id\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","import pandas as pd\n","# Authenticate the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"metadata":{"id":"CcTorQ2YGBG3","executionInfo":{"status":"ok","timestamp":1671218503823,"user_tz":-180,"elapsed":20174,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Defs"],"metadata":{"id":"10lTaFIDAtky"}},{"cell_type":"code","execution_count":10,"metadata":{"ExecuteTime":{"end_time":"2022-11-28T13:35:50.676255Z","start_time":"2022-11-28T13:35:50.634006Z"},"id":"Opcbq6SN7HY_","executionInfo":{"status":"ok","timestamp":1671218503824,"user_tz":-180,"elapsed":16,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"outputs":[],"source":["#Function to load files of different companies\n","def get_compData(comp):\n","  if comp =='AAPL':\n","    # AAPL: https://drive.google.com/file/d/1diLooyj8DtyyxwiihpbTJdfY4D98irLE/view?usp=sharing\n","    id = '1diLooyj8DtyyxwiihpbTJdfY4D98irLE'\n","    downloaded1 = drive.CreateFile({'id':id}) \n","    downloaded1.GetContentFile('stocktwits_AAPL.csv')\n","    df_st1 = pd.read_csv('stocktwits_AAPL.csv')\n","    return df_st1\n","  elif comp=='ADBE':\n","    # ADBE: https://drive.google.com/file/d/1SMyRg8aUnnQTbukVadBo814qdS6xDnP1/view?usp=sharing\n","    id = '1SMyRg8aUnnQTbukVadBo814qdS6xDnP1'\n","    downloaded2 = drive.CreateFile({'id':id}) \n","    downloaded2.GetContentFile('stocktwits_ADBE.csv')\n","    df_st2 = pd.read_csv('stocktwits_ADBE.csv')\n","    return df_st2\n","  elif comp=='AMZN':\n","    # AMZN: https://drive.google.com/file/d/1ffmFOrlcaTCCByKMq7E1LCXXK6daMkB_/view?usp=sharing\n","    id = '1ffmFOrlcaTCCByKMq7E1LCXXK6daMkB_'\n","    downloaded3 = drive.CreateFile({'id':id}) \n","    downloaded3.GetContentFile('stocktwits_AMZN.csv')\n","    df_st3 = pd.read_csv('stocktwits_AMZN.csv')\n","    return df_st3\n","  elif comp=='BAC':\n","    # BAC: https://drive.google.com/file/d/1HeaKJtlSz2xiLT3sx5FLdYmzCkVDGIjW/view?usp=sharing\n","    id = '1HeaKJtlSz2xiLT3sx5FLdYmzCkVDGIjW'\n","    downloaded4 = drive.CreateFile({'id':id}) \n","    downloaded4.GetContentFile('stocktwits_BAC.csv')\n","    df_st4 = pd.read_csv('stocktwits_BAC.csv')\n","    return df_st4\n","\n","  elif comp=='BRK.A':\n","    #BRK_A: https://drive.google.com/file/d/1HeQhAU20YyT1tRCD-yN83vrUGY7jDDSt/view?usp=sharing\n","    id = '1HeQhAU20YyT1tRCD-yN83vrUGY7jDDSt'\n","    downloaded5 = drive.CreateFile({'id':id}) \n","    downloaded5.GetContentFile('stocktwits_BRK_A.csv')\n","    df_st5 = pd.read_csv('stocktwits_BRK_A.csv')\n","    return df_st5\n","  elif comp=='BRK.B':\n","    #BRK_B: https://drive.google.com/file/d/1FRrwrIVJVyFqg025SWasKW84qOJMhe84/view?usp=sharing\n","    id = '1FRrwrIVJVyFqg025SWasKW84qOJMhe84'\n","    downloaded6 = drive.CreateFile({'id':id}) \n","    downloaded6.GetContentFile('stocktwits_BRK_B.csv')\n","    df_st6 = pd.read_csv('stocktwits_BRK_B.csv')\n","    return df_st6\n","  elif comp=='DIA':\n","    #DIA: https://drive.google.com/file/d/1riZ8IdkupLre9NQ7McBO21wDFVX_NPvg/view?usp=sharing\n","    id = '1riZ8IdkupLre9NQ7McBO21wDFVX_NPvg'\n","    downloaded7 = drive.CreateFile({'id':id}) \n","    downloaded7.GetContentFile('stocktwits_DIA.csv')\n","    df_st7 = pd.read_csv('stocktwits_DIA.csv')\n","    return df_st7\n","  elif comp=='DIS':\n","    #DIS: https://drive.google.com/file/d/1y5IT3gA_yIJnFLTsxy1GHdIY4SHwj84Y/view?usp=sharing\n","    id = '1y5IT3gA_yIJnFLTsxy1GHdIY4SHwj84Y'\n","    downloaded8 = drive.CreateFile({'id':id}) \n","    downloaded8.GetContentFile('stocktwits_DIS.csv')\n","    df_st8 = pd.read_csv('stocktwits_DIS.csv')\n","    return df_st8\n","  elif comp=='FB':\n","    #FB: https://drive.google.com/file/d/1x1FugJhUQx9xKWS9LYnio1MU8oi5iuTc/view?usp=sharing\n","    id = '1x1FugJhUQx9xKWS9LYnio1MU8oi5iuTc'\n","    downloaded9 = drive.CreateFile({'id':id}) \n","    downloaded9.GetContentFile('stocktwits_FB.csv')\n","    df_st9 = pd.read_csv('stocktwits_FB.csv')\n","    return df_st9\n","  elif comp=='GOOG':\n","    #GOOG: https://drive.google.com/file/d/1UMnPXfG_XkgLJAQ2VtLvMPBPZhN68Ezw/view?usp=sharing\n","    id = '1UMnPXfG_XkgLJAQ2VtLvMPBPZhN68Ezw'\n","    downloaded10 = drive.CreateFile({'id':id}) \n","    downloaded10.GetContentFile('stocktwits_GOOG.csv')\n","    df_st10 = pd.read_csv('stocktwits_GOOG.csv')\n","    return df_st10\n","  elif comp=='GOOGL':\n","    #GOOGL: https://drive.google.com/file/d/1NbCpQX0sTgXsqcW7xMbkrDPlAOscIoTL/view?usp=sharing\n","    id = '1NbCpQX0sTgXsqcW7xMbkrDPlAOscIoTL'\n","    downloaded11 = drive.CreateFile({'id':id}) \n","    downloaded11.GetContentFile('stocktwits_GOOGL.csv')\n","    df_st11 = pd.read_csv('stocktwits_GOOGL.csv')\n","    return df_st11\n","  elif comp=='HD':\n","    #HD: https://drive.google.com/file/d/10wbCzJMcjLWAqrlEtHlWieIrq7dyolG2/view?usp=sharing\n","    id = '10wbCzJMcjLWAqrlEtHlWieIrq7dyolG2'\n","    downloaded12 = drive.CreateFile({'id':id}) \n","    downloaded12.GetContentFile('stocktwits_HD.csv')\n","    df_st12 = pd.read_csv('stocktwits_HD.csv')\n","    return df_st12\n","  elif comp=='INTC':\n","    # INTC: https://drive.google.com/file/d/1k1-NSl8qLTa2oDs1G8CFC4CJzkv9mugw/view?usp=sharing \n","    id = '1k1-NSl8qLTa2oDs1G8CFC4CJzkv9mugw'\n","    downloaded13 = drive.CreateFile({'id':id}) \n","    downloaded13.GetContentFile('stocktwits_INTC.csv')\n","    df_st13 = pd.read_csv('stocktwits_INTC.csv')\n","    return df_st13\n","  elif comp=='JNJ':\n","    # JNJ: https://drive.google.com/file/d/1Qiwu9vbDYU527szR8waaFDoFCyY4i_bc/view?usp=sharing\n","    id = '1Qiwu9vbDYU527szR8waaFDoFCyY4i_bc'\n","    downloaded14 = drive.CreateFile({'id':id}) \n","    downloaded14.GetContentFile('stocktwits_JNJ.csv')\n","    df_st14 = pd.read_csv('stocktwits_JNJ.csv')\n","    return df_st14\n","  elif comp=='NFLX':\n","    # NFLX: https://drive.google.com/file/d/1DdJ8MPdgt9bxF3ZagkWUp45N4RMQ8Al6/view?usp=sharing\n","    id = '1DdJ8MPdgt9bxF3ZagkWUp45N4RMQ8Al6'\n","    downloaded15 = drive.CreateFile({'id':id}) \n","    downloaded15.GetContentFile('stocktwits_NFLX.csv')\n","    df_st15 = pd.read_csv('stocktwits_NFLX.csv')\n","    return df_st15\n","  elif comp=='PG':\n","    # PG: https://drive.google.com/file/d/1tlueLaJhlduNMgRHk8Omkog5nNI8I8Yk/view?usp=sharing\n","    id = '1tlueLaJhlduNMgRHk8Omkog5nNI8I8Yk'\n","    downloaded16 = drive.CreateFile({'id':id}) \n","    downloaded16.GetContentFile('stocktwits_PG.csv')\n","    df_st16 = pd.read_csv('stocktwits_PG.csv')\n","    return df_st16\n","  elif comp=='QQQ':\n","    # QQQ: https://drive.google.com/file/d/1gUsl5L4VBgsL9oqBxaUdk6tA9r4VxDjo/view?usp=sharing\n","    id = '1gUsl5L4VBgsL9oqBxaUdk6tA9r4VxDjo'\n","    downloaded17 = drive.CreateFile({'id':id}) \n","    downloaded17.GetContentFile('stocktwits_QQQ.csv')\n","    df_st17 = pd.read_csv('stocktwits_QQQ.csv')\n","    return df_st17\n","  elif comp=='SPY':\n","    # SPY: https://drive.google.com/file/d/10s-zYQPIqlkNsUahgzDRqJGw0VAkn-R1/view?usp=sharing\n","    id = '10s-zYQPIqlkNsUahgzDRqJGw0VAkn-R1'\n","    downloaded18 = drive.CreateFile({'id':id}) \n","    downloaded18.GetContentFile('stocktwits_SPY.csv')\n","    df_st18 = pd.read_csv('stocktwits_SPY.csv')\n","    return df_st18\n","  elif comp=='T':\n","    # T: https://drive.google.com/file/d/1rk3PsikhgrA7MxV28tUbzj7EOn9K5Ixu/view?usp=sharing\n","    id = '1rk3PsikhgrA7MxV28tUbzj7EOn9K5Ixu'\n","    downloaded19 = drive.CreateFile({'id':id}) \n","    downloaded19.GetContentFile('stocktwits_T.csv')\n","    df_st19 = pd.read_csv('stocktwits_T.csv')\n","    return df_st19\n","  elif comp=='TSLA':\n","    # TSLA: https://drive.google.com/file/d/1on57uk2gd_CLsnj1dcRfuB_KsYydzEp2/view?usp=sharing\n","    id = '1on57uk2gd_CLsnj1dcRfuB_KsYydzEp2'\n","    downloaded20 = drive.CreateFile({'id':id}) \n","    downloaded20.GetContentFile('stocktwits_TSLA.csv')\n","    df_st20 = pd.read_csv('stocktwits_TSLA.csv')\n","    return df_st20\n","  elif comp=='UNH':\n","    # UNH: https://drive.google.com/file/d/1zguMHb3pL2tCT4TYV8XJcP-dWTcq28mY/view?usp=sharing\n","    id = '1zguMHb3pL2tCT4TYV8XJcP-dWTcq28mY'\n","    downloaded21 = drive.CreateFile({'id':id}) \n","    downloaded21.GetContentFile('stocktwits_UNH.csv')\n","    df_st21 = pd.read_csv('stocktwits_UNH.csv')\n","    return df_st21\n","  elif comp=='V':\n","    # V: https://drive.google.com/file/d/1qLI1Rsyf2ebZu53I8QaFuOQSeTyZShLg/view?usp=sharing\n","    id = '1qLI1Rsyf2ebZu53I8QaFuOQSeTyZShLg'\n","    downloaded22 = drive.CreateFile({'id':id}) \n","    downloaded22.GetContentFile('stocktwits_V.csv')\n","    df_st22 = pd.read_csv('stocktwits_V.csv')\n","    return df_st22\n","  elif comp=='VIX':\n","    # VIX: https://drive.google.com/file/d/1SoIue0nfsn_GGMOroFg3tEp8re-v7PnJ/view?usp=sharing\n","    id = '1SoIue0nfsn_GGMOroFg3tEp8re-v7PnJ'\n","    downloaded23 = drive.CreateFile({'id':id}) \n","    downloaded23.GetContentFile('stocktwits_VIX.csv')\n","    df_st23 = pd.read_csv('stocktwits_VIX.csv')\n","    return df_st23\n","  elif comp=='VZ':\n","    # VZ: https://drive.google.com/file/d/1ddISbB0qfDpM69senqEmmf6xbNWOpNUJ/view?usp=sharing\n","    id = '1ddISbB0qfDpM69senqEmmf6xbNWOpNUJ'\n","    downloaded24 = drive.CreateFile({'id':id}) \n","    downloaded24.GetContentFile('stocktwits_VZ.csv')\n","    df_st24 = pd.read_csv('stocktwits_VZ.csv')\n","    return df_st24\n","  elif comp=='WMT':\n","    # WMT: https://drive.google.com/file/d/14Zdh1ZCj5RxZltXknG3Qisxhwzge5rkV/view?usp=sharing\n","    id = '14Zdh1ZCj5RxZltXknG3Qisxhwzge5rkV'\n","    downloaded25 = drive.CreateFile({'id':id}) \n","    downloaded25.GetContentFile('stocktwits_WMT.csv')\n","    df_st25 = pd.read_csv('stocktwits_WMT.csv')\n","    return df_st25"]},{"cell_type":"code","source":[],"metadata":{"id":"JUPuqln_XokF","executionInfo":{"status":"ok","timestamp":1671218503826,"user_tz":-180,"elapsed":16,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def get_tweets_df_for_ticker(ticker: str, first_quote_date_str: str = None, verbose=True) -> pd.DataFrame:\n","    print(f\"Getting data for ticker: {ticker} and first_quote_date_str: {first_quote_date_str}\")\n","    df = get_compData(ticker)\n","    if verbose:\n","      print(f\"\\nHead:\\n\", df.head())\n","      print(f\"\\nTail:\\n\", df.tail())\n","\n","    #date time split code\n","    #df['datetime'] = df['datetime'].astype('datetime64[ns]') #len(df_st1['datetime'])\n","    df['datetime'] = pd.to_datetime(df['datetime'], errors='raise')\n","    df['Date'] = pd.to_datetime([d.date() for d in df['datetime']], errors='raise')\n","    # df['Time'] = [d.time() for d in df['datetime']]  # Cannot convert to datetime type\n","    df['Weekday'] = [d.date().weekday() for d in df['Date']]\n","\n","    # \n","    if first_quote_date_str is not None:\n","        first_quote_date =  datetime.strptime(first_quote_date_str, \"%Y-%m-%d\").astimezone(timezone.utc)  # tz_localize('utc')\n","        df = df[df.datetime >= first_quote_date]\n","    if verbose:\n","        print(f\"\\nTail\\n\", df.tail())\n","\n","    return df"],"metadata":{"id":"TzL74CSCBLln","executionInfo":{"status":"ok","timestamp":1671218503827,"user_tz":-180,"elapsed":15,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def get_yahoo_data_for_df(df: pd.DataFrame, left_border_days=1, right_border_days=1, yf_protection_days=3, \n","                          verbose=True) -> pd.DataFrame:\n","\n","    start_dt = df['Date'].min() + timedelta(days=-left_border_days)\n","    end_dt = df['Date'].max() + timedelta(days=right_border_days)\n","    compny = df['symbol'].iloc[0]\n","    if compny =='BRK.A':\n","      compny = 'BRK-A'\n","    elif compny =='BRK.B':\n","      compny ='BRK-B'\n","    elif compny =='VIX':\n","      compny = '^VIX'\n","    elif compny == 'FB':\n","      compny = 'META'\n","    # elif compny == 'GOOG':\n","    #   compny = 'GOOGL'  # Data from 2004 instead of \n","    else:\n","      pass\n","\n","    if verbose:\n","        print(f\"Start gettings data for {compny} in range {start_dt} to {end_dt}\")\n","    # Request for end_dt + 1 as YF does not inlclude right border.\n","    # Note: auto_adjust defaults to False, but in fact all prices are autoadjusted even if it is False (?)\n","    # Note: keepna=True seems not to fill weekends, etc. (?)\n","    prot_delta = timedelta(days=yf_protection_days)\n","    yahoo_data = yfinance.download(\n","        compny, start=start_dt - prot_delta, end=end_dt + prot_delta, keepna=True, auto_adjust=True)\n","    yahoo_data.reset_index(level=0, inplace=True)\n","    print('Columns:', yahoo_data.columns)\n","    print('Shape:', yahoo_data.shape)\n","\n","    if verbose:\n","        print('\\nHead:\\n', yahoo_data.head())\n","        print('\\nInfo:')\n","        yahoo_data.info()\n","        print('\\nDescribe:\\n', yahoo_data.describe(include='all', datetime_is_numeric=True))\n","\n","    # Check dates (probably, in case of holidays, etc. the assertions may fail) -> some protection dates should be used\n","    print(\"T1525\")\n","    assert yahoo_data['Date'].min() <= start_dt, \\\n","        f\"Not enough data obtained: {yahoo_data['Date'].min()} vs {start_dt} ({start_dt:%A})\"\n","    assert yahoo_data['Date'].max() >= end_dt, \\\n","        f\"Not enough data obtained: {yahoo_data['Date'].max()} vs {end_dt} ({end_dt:%A})\"\n","\n","    # Check if Close and Adjusted Close are the same (new change in yfinance?)\n","    #assert np.allclose(yahoo_data['Close'], yahoo_data['Adj Close'], equal_nan=True)\n","    assert 'Adj Close' not in yahoo_data.columns  # Due to auto_adjust=True param\n","\n","    return yahoo_data"],"metadata":{"id":"ifnUyQ69CajQ","executionInfo":{"status":"ok","timestamp":1671218503829,"user_tz":-180,"elapsed":16,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def fill_missing_dates(yf_df: pd.DataFrame, verbose=True) -> pd.DataFrame:\n","    # Check data types\n","    assert yf_df.columns.to_list() == ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n","    assert np.issubdtype(yf_df.index.dtype, np.integer)\n","    assert np.issubdtype(yf_df.Date.dtype, np.datetime64)\n","    if verbose:\n","        print('\\nBefore reindexing:\\n', yf_df.head())\n","\n","    # Set \"Date\" column as index with filling gaps with NaNs\n","    new_df = yf_df.set_index('Date').asfreq('D')  #.reset_index()\n","    assert type(new_df.index) == pd.DatetimeIndex\n","    if verbose:\n","        print('\\nAfter reindexing:\\n', new_df.head())\n","\n","    # Fill NaN values in the 'Close' column by ffill strategy\n","    new_df['Close'] = new_df['Close'].fillna(method='ffill')\n","    if verbose:\n","        print('\\nAfter ffill for NAs in Close column:\\n', new_df.head())\n","\n","    # Fill NaN values in other columns from the 'Close' column\n","    new_df['Open'] = new_df['Open'].fillna(new_df['Close'])\n","    new_df['High'] = new_df['High'].fillna(new_df['Close'])\n","    new_df['Low'] = new_df['Low'].fillna(new_df['Close'])\n","    if verbose:\n","        print('\\nAfter filling NAs in other columns:\\n', new_df.head())\n","\n","    print(f'Dates count: {len(yf_df)} -> {len(new_df)}')\n","    return new_df"],"metadata":{"id":"3Uz4b4zEl2KK","executionInfo":{"status":"ok","timestamp":1671218504160,"user_tz":-180,"elapsed":346,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def get_tweets_with_price_raw_data(df_tweets: pd.DataFrame, df_prices: pd.DataFrame, \n","                                   day_shifts = [-1, 0, 1, 2, 3, 4, 5, 6, 7], verbose=True) -> pd.DataFrame:\n","\n","    # Check types, etc.\n","    assert df_tweets.columns.to_list() == ['symbol', 'message', 'datetime', 'user', 'message_id', 'Date', 'Weekday']\n","    assert np.issubdtype(df_tweets.Date.dtype, np.datetime64)\n","\n","    assert df_prices.columns.to_list() == ['Open', 'High', 'Low', 'Close', 'Volume']\n","    assert np.issubdtype(df_prices.index.dtype, np.datetime64)\n","    \n","    # Fast check if min date for prices is sufficient\n","    min_tweet_date = df_tweets.Date.min()\n","    expected_min_price_date = min_tweet_date + timedelta(days=min(day_shifts))\n","    actual_min_price_date = df_prices.index.min()\n","    assert actual_min_price_date <= expected_min_price_date, \\\n","        f\"Not enough dates in price: {actual_min_price_date} instead of {expected_min_price_date}\"\n","\n","    # Fast check if max date for prices is sufficient\n","    max_tweet_date = df_tweets.Date.max()\n","    expected_max_price_date = max_tweet_date + timedelta(days=max(day_shifts))\n","    actual_max_price_date = df_prices.index.max()\n","    assert actual_max_price_date >= expected_max_price_date, \\\n","        f\"Not enough dates in price: {actual_max_price_date} instead of {expected_max_price_date}\"\n","\n","    # Cycle for each tweet\n","    tmp_dict = defaultdict(list)\n","\n","    for index, row in tqdm(df_tweets.iterrows(), total=len(df_tweets)):\n","        tweet_date = row.Date\n","        tmp_list = []\n","        for shift in day_shifts:\n","            price_row = df_prices.loc[tweet_date + timedelta(days=shift)]\n","            assert isinstance(price_row, pd.Series)\n","            tmp_dict[f\"d{shift}_O\"].append(price_row['Open'])\n","            tmp_dict[f\"d{shift}_H\"].append(price_row['High'])\n","            tmp_dict[f\"d{shift}_L\"].append(price_row['Low'])\n","            tmp_dict[f\"d{shift}_C\"].append(price_row['Close'])\n","            tmp_dict[f\"d{shift}_V\"].append(price_row['Volume'])\n","\n","    out_df = df_tweets.copy()\n","    for col_name, col_data in tmp_dict.items():\n","        out_df[col_name] = col_data\n","\n","    if verbose:\n","        print('\\nFinal head:\\n', out_df.head())\n","\n","    return out_df"],"metadata":{"id":"hL4K-9Mc3rck","executionInfo":{"status":"ok","timestamp":1671218504161,"user_tz":-180,"elapsed":10,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"KVmMMQ_WsTsz","executionInfo":{"status":"ok","timestamp":1671218504161,"user_tz":-180,"elapsed":8,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"outputs":[],"source":["def get_label(ch):\n","  if ch>0.5:\n","    return 1\n","  elif ch<-0.5:\n","    return -1\n","  else:\n","    return 0"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"C65MA9lJnwEv","executionInfo":{"status":"ok","timestamp":1671218504162,"user_tz":-180,"elapsed":8,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"outputs":[],"source":["def remove_stopwords(msg_seg: list, stop_words: list):\n","    assert isinstance(msg_seg, list)\n","    filtered_sentence = [w for w in msg_seg if not w in stop_words]\n","    return filtered_sentence\n","\n","def remove_punctuation_re(x):\n","    x = ' '.join(re.sub(\"https?://\\S+\",\"\",x).split())     #Removing URLs\n","\n","    x = ' '.join(re.sub(\"^@\\S+|\\s@\\S+\",\"\",x).split())     #Removing Mentions\n","\n","    # x = ' '.join(re.sub(r'[^$\\w\\s]',\" \",x).split())\n","    x = ' '.join(re.sub(r'[^\\w\\s]',\" \",x).split())        #Removes Hashtags\n","\n","    x = ' '.join(re.sub(r'_',\" \",x).split())              #Removing _ from emojis text\n","\n","    return x\n","\n","# replace repeating letter\n","def do_rpt_replace(match):\n","    # print(match.group(1))\n","\n","    return match.group(1)+match.group(1)\n","\n","# for repeating characters in words\n","RE_MESSAGE_RPT = re.compile(r\"(.)\\1{2,}\", re.IGNORECASE)\n","\n","# substitute original word with replaced word, if any\n","def processRepeatings(data):\n","    # print('RPT1:', data)\n","    re_t= re.sub(RE_MESSAGE_RPT, do_rpt_replace, data )\n","    # print(re_t)\n","    #print('RPT2:', re_t)\n","    return re_t"]},{"cell_type":"code","source":["def get_preprocessed_tweets_df(df_tweets: pd.DataFrame, to_remove_stopwords: bool, to_remove_repetitions: bool, \n","                               verbose=True) -> pd.DataFrame:\n","\n","    # Prepare processors\n","    stop_words = sw.words(\"english\")\n","    tweet_tokenizer = TweetTokenizer()\n","    detokenizer = TreebankWordDetokenizer()\n","    # segmenter using the word statistics from Twitter\n","    seg_tw = Segmenter(corpus=\"twitter\")\n","\n","\n","    out_df = df_tweets.copy()\n","    # fill nan values in file with '0'\n","    # df_tweets.isna().values.any()\n","    out_df['message'] = out_df['message'].fillna('0')\n","\n","    out_df['message'] = out_df['message'].str.lower()\n","    messages = out_df['message'].tolist()\n","\n","    message_p = []\n","    for i, msg in tqdm(enumerate(messages), total=len(messages)):\n","        print(\"Orig:\", msg) if verbose else None\n","\n","        if msg == '0': #nan replaced by '0'\n","            message_p.append('-1')\n","\n","        else:\n","            # remove emojis\n","            msg = emoji.demojize(msg)\n","\n","            # fix contractions\n","            msg = contractions.fix(msg)\n","\n","            # remove punctuations\n","            msg = remove_punctuation_re(msg) \n","\n","            #tokenize\n","            msg_tokens = tweet_tokenizer.tokenize(msg)\n","\n","            #For Hashtags elongated words using Word segmenter\n","            message_seg = []\n","            for w in msg_tokens:\n","              if len(w)>=300:\n","                w=w[:100]\n","                print(w)\n","              message_seg.append(seg_tw.segment(w))\n","\n","            # remove stopwords\n","            if to_remove_stopwords:\n","                msg_list = remove_stopwords(message_seg, stop_words=stop_words)\n","            else:\n","                msg_list = message_seg\n","\n","            if 'rt' in msg_list:\n","                # remove retweets\n","                message_p.append('-1')\n","            else: \n","                # detokenize\n","                msg = detokenizer.detokenize(msg_list)\n","\n","                # removing repeating characters like hurrrryyyyyy-- worrks on tokenized list\n","                if to_remove_repetitions:\n","                    msg = processRepeatings(msg)\n","\n","                message_p.append(msg)      \n","\n","        print(\"Final:\", msg, \"\\n\") if verbose else None\n","\n","        if i == 10 and verbose:\n","            break\n","\n","    out_df['message'] = message_p\n","    \n","    # Drop Retweets\n","    out_df.drop(out_df[out_df['message'] == '-1'].index, inplace = True)     \n","    \n","    return out_df"],"metadata":{"id":"VrlDewgGsAZu","executionInfo":{"status":"ok","timestamp":1671218504163,"user_tz":-180,"elapsed":9,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# Settings"],"metadata":{"id":"TipkIy2b-vJz"}},{"cell_type":"code","source":["# Main settings\n","\n","# Pairs of \"ticker\" - \"first quote date\"\n","SRC_TICKERS_DATES_PAIRS = [\n","    ('AAPL', None), \n","    ('AMZN', None), \n","    ('FB', '2012-06-01'),    # Note: data from 2012 (?), in 2021 changed to META, but here the data is up to 2020-07.\n","    ('GOOGL', None),  # Quotes in NASDAQ since 2004\n","    ('GOOG', None),  # Quotes in NASDAQ since 2014\n","    ('NFLX', None),\n","    ]\n","\n","MAX_SHIFT_DAYS = 7  # Days shift will range from -1 to MAX_SHIFT_DAYS\n","  \n","TO_REMOVE_STOPWORDS = False  # sw.words(\"english\") contains about 200 tokens insluding \"not\", \"don't\", etc.\n","TO_REMOVE_REPETITIONS = False\n","\n","OUT_FILE_ADDITIONAL_SUFFIX = f\"_1y\"\n","OUT_FLOAT_FORMAT = '%.2f'\n","\n","#change the directory you want to save the file\n","GD_OUT_PATH = r'/content/drive/MyDrive/_PR_ROOT/_2022/2022-11_NLP-Huawei_Final_project/stocktwits_finsentiment_analysis/data/interim/040_output__nb010_v1/'\n","assert os.path.isdir(GD_OUT_PATH)"],"metadata":{"id":"DDUw_RrV-ZyJ","executionInfo":{"status":"ok","timestamp":1671218505311,"user_tz":-180,"elapsed":1156,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# Additional settings\n","YF_PROTECTION_DAYS = 3  # For some reason, yfinance sometimes gets not exact date borderes"],"metadata":{"id":"OhaxTLXQUCti","executionInfo":{"status":"ok","timestamp":1671218505311,"user_tz":-180,"elapsed":5,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nePsBDOKPGvi","executionInfo":{"status":"ok","timestamp":1671218505312,"user_tz":-180,"elapsed":5,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# Dump all tickers (local csv files will be created)"],"metadata":{"id":"9hE0yyGsYBn8"}},{"cell_type":"code","source":["tickers_25 = [\n","'AAPL',\n","'ADBE',\n","'AMZN',\n","'BAC',\n","'BRK.A',\n","'BRK.B',\n","'DIA',\n","'DIS',\n","'FB',\n","'GOOG',\n","'GOOGL',\n","'HD',\n","'INTC',\n","'JNJ',\n","'NFLX',\n","'PG',\n","'QQQ',\n","'SPY',\n","'T',\n","'TSLA',\n","'UNH',\n","'V',\n","'VIX',\n","'VZ',\n","'WMT',\n","]\n"],"metadata":{"id":"2MJNlPSXYImn","executionInfo":{"status":"ok","timestamp":1671218505312,"user_tz":-180,"elapsed":4,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# for t in tickers_25:\n","#     _ = get_compData(t)"],"metadata":{"id":"EmtUBD26YMBH","executionInfo":{"status":"ok","timestamp":1671218706733,"user_tz":-180,"elapsed":201425,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# !cp *.csv drive/MyDrive/_PR_ROOT/_2022/2022-11_NLP-Huawei_Final_project/Datasets/2022-12-16__raw_csvs_25_tickers/"],"metadata":{"id":"TDkio28NZKOW","executionInfo":{"status":"ok","timestamp":1671218747173,"user_tz":-180,"elapsed":4957,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# assert False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":172},"id":"ECQhLB_LYXJM","executionInfo":{"status":"error","timestamp":1671218707043,"user_tz":-180,"elapsed":315,"user":{"displayName":"AnT","userId":"07342426211356883844"}},"outputId":"3c8bd424-a6db-4174-b3e0-6c093859652d"},"execution_count":22,"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-a871fdc9ebee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAssertionError\u001b[0m: "]}]},{"cell_type":"markdown","source":["# Main cycle for tickers"],"metadata":{"id":"cVfedRH-avJ7"}},{"cell_type":"code","source":["FAST_CHECK = False\n","VERBOSE = True \n","sep = '*' * 100 \n","\n","tickers = [('VIX', None)] if FAST_CHECK else SRC_TICKERS_DATES_PAIRS[2:]\n","\n","for ticker, first_quote_date_str in tickers:\n","  print(f\"{sep}\\n* Step1: get_tweets_df_for_ticker\\n{sep}\")\n","  df_tweets = get_tweets_df_for_ticker(ticker, first_quote_date_str, verbose=True)\n","  \n","  print(f\"{sep}\\n* Step2: get_yahoo_data_for_df\\n{sep}\")\n","  df_yf = get_yahoo_data_for_df(df_tweets, right_border_days=MAX_SHIFT_DAYS, verbose=True)\n","\n","  print(f\"{sep}\\n* Step3: fill_missing_dates\\n{sep}\")\n","  df_yf2 = fill_missing_dates(df_yf, verbose=True)\n","\n","  print(f\"{sep}\\n* Step4: get_tweets_with_price_raw_data\\n{sep}\")\n","  if FAST_CHECK:\n","      df_tweets = df_tweets[:1000]\n","  df_tweets_with_price = get_tweets_with_price_raw_data(\n","      df_tweets, df_prices=df_yf2, day_shifts=list(range(-1, MAX_SHIFT_DAYS + 1)), verbose=True)\n","\n","  print(f\"{sep}\\n* Step5: get_preprocessed_tweets_df\\n{sep}\")\n","  df_final = get_preprocessed_tweets_df(df_tweets_with_price, to_remove_stopwords=TO_REMOVE_STOPWORDS, \n","                                        to_remove_repetitions=TO_REMOVE_REPETITIONS, verbose=False)\n","\n","  # Save to file\n","  print(f\"{sep}\\n* Step6: save to file\\n{sep}\")\n","  ts = datetime.now().strftime('%Y-%m-%dT%H%M%S')\n","  sw_tag = f\"RmSW={int(TO_REMOVE_STOPWORDS)}\"\n","  rep_tag = f\"RmRep={int(TO_REMOVE_REPETITIONS)}\"\n","  out_file_name = f\"{ts}_{ticker}_{sw_tag}_{rep_tag}{OUT_FILE_ADDITIONAL_SUFFIX}.csv.gz\"\n","  df_final.to_csv(GD_OUT_PATH + out_file_name, header=True, index=False, \n","                  encoding='utf_8', float_format=OUT_FLOAT_FORMAT, compression='gzip')\n","  print(\"Success\")"],"metadata":{"id":"LG_zTtBerGN8","executionInfo":{"status":"aborted","timestamp":1671218707045,"user_tz":-180,"elapsed":9,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reading twitter - 1grams ...\n","# Reading twitter - 2grams ...\n","# 10/? [00:00<00:00, 9.65it/s]\n","# T1758: 0\n","# Orig: $uvxy $vxx $svxy $spy if you absolutely must look at stochastics for the $vix these are the ones and only ones to monitor. everything is left to derivate/reaction principal functions. any questions on exactly what the stochastics are/mean, ask!\n","# RPT1: uvxy vxx svxy spy absolutely must look stochastic s vix ones ones monitor everything left der iv ate reaction principal functions questions exactly stochastic s mean ask\n","# RPT2: uvxy vxx svxy spy absolutely must look stochastic s vix ones ones monitor everything left der iv ate reaction principal functions questions exactly stochastic s mean ask\n","# Final: uvxy vxx svxy spy absolutely must look stochastic s vix ones ones monitor everything left der iv ate reaction principal functions questions exactly stochastic s mean ask \n","\n","# T1758: 1\n","# Orig: $tsla  $1610 put 8/7/20  - 1 contract at $162.30.  $amzn  $2400 10/16/20 - 1 contract at $33.40.  otm for amzn because it is all i had left in my account.  parabolas have to come down sometimes.  i believe fundamentals will come into view this week (ie real earnings). $vix seems to be going higher.\n","# RPT1: tsla  1610 put  8  7  20  1 contract  162  30 amzn  2400  10  16  20  1 contract  33  40 otm amzn left account parabolas come sometimes believe fundamentals come view week i e real earnings vix seems going higher\n","# RPT2: tsla  1610 put  8  7  20  1 contract  162  30 amzn  2400  10  16  20  1 contract  33  40 otm amzn left account parabolas come sometimes believe fundamentals come view week i e real earnings vix seems going higher\n","# Final: tsla  1610 put  8  7  20  1 contract  162  30 amzn  2400  10  16  20  1 contract  33  40 otm amzn left account parabolas come sometimes believe fundamentals come view week i e real earnings vix seems going higher \n","\n","# T1758: 2\n","# Orig: $spy $aapl $vix $goog $amzn \n","\n","# can&#39;t wait to make $$$ tomorrow, too many cuck bears in disbelief after buying those 325s atm. let me know how that goes tomorrow fuckwits. it&#39;s a good thing you all suck at math otherwise i wouldn&#39;t be retired.\n","\n","# https://www.youtube.com/watch?v=gvymcvrijbo\n","# RPT1: spy aapl vix goog amzn  39 wait make tomorrow many cuck bears disbelief buying 325s atm let know goes tomorrow fuckwits  39 good thing suck math otherwise  39 retired\n","# RPT2: spy aapl vix goog amzn  39 wait make tomorrow many cuck bears disbelief buying 325s atm let know goes tomorrow fuckwits  39 good thing suck math otherwise  39 retired\n","# Final: spy aapl vix goog amzn  39 wait make tomorrow many cuck bears disbelief buying 325s atm let know goes tomorrow fuckwits  39 good thing suck math otherwise  39 retired \n","\n","# T1758: 3\n","# Orig: $spy btmfd!\n","# $vix dip that is\n","# RPT1: spy bt mfd vix dip\n","# RPT2: spy bt mfd vix dip\n","# Final: spy bt mfd vix dip \n","\n","# T1758: 4\n","# Orig: $vix 5 red days and then rdr green. some caution\n","# RPT1: vix  5 red days rdr green caution\n","# RPT2: vix  5 red days rdr green caution\n","# Final: vix  5 red days rdr green caution \n","\n","# T1758: 5\n","# Orig: $uxvy $vix daily chart updated ... oh oh ...\n","# RPT1: u xv y vix daily chart updated oh oh\n","# RPT2: u xv y vix daily chart updated oh oh\n","# Final: u xv y vix daily chart updated oh oh \n","\n","# T1758: 6\n","# Orig: $uvxy  how come only up 1% when $vix is up 4.5%?\n","# RPT1: uvxy come  1 vix  4  5\n","# RPT2: uvxy come  1 vix  4  5\n","# Final: uvxy come  1 vix  4  5 \n","\n","# T1758: 7\n","# Orig: $spy $vix the turn has wormed.\n","# RPT1: spy vix turn wormed\n","# RPT2: spy vix turn wormed\n","# Final: spy vix turn wormed \n","\n","# T1758: 8\n","# Orig: nyse advance-decline line breaking out to all-time high... \n","\n","# another major bullish evidence. \n","\n","# $spx $spy $qqq $dia $vix\n","# RPT1: nyse advance decline line breaking time high another major bullish evidence spx spy qqq dia vix\n","# RPT2: nyse advance decline line breaking time high another major bullish evidence spx spy qq dia vix\n","# Final: nyse advance decline line breaking time high another major bullish evidence spx spy qq dia vix \n","\n","# T1758: 9\n","# Orig: $spy got legs, $vix fading\n","# RPT1: spy got legs vix fading\n","# RPT2: spy got legs vix fading\n","# Final: spy got legs vix fading "],"metadata":{"id":"H6VPxUkz66MR","executionInfo":{"status":"aborted","timestamp":1671218707046,"user_tz":-180,"elapsed":9,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KkSpMpJWAG4u","executionInfo":{"status":"aborted","timestamp":1671218707047,"user_tz":-180,"elapsed":10,"user":{"displayName":"AnT","userId":"07342426211356883844"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}